{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOs4c2bWhBrE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install pymorphy2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFvY8emBhETx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install contractions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gosKqtPhM0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install pyaspeller"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHFjyRfflMd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install contractions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rwavHfbkN7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn import datasets\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.externals import joblib\n",
        "import gensim\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('white')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "%matplotlib inline\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "import nltk\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "#from contractions import contractions_dict\n",
        "#import contractions\n",
        "import unicodedata\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.externals import joblib\n",
        "import json\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "#stopwords.words('russian')\n",
        "import pymorphy2\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "from pyaspeller import YandexSpeller\n",
        "import nltk\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import pymorphy2\n",
        "from nltk.corpus import stopwords\n",
        "#from stop_words import get_stop_words\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import os \n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.externals import joblib\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEyw4QGAlRBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('white')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "%matplotlib inline\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "import nltk\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from contractions import contractions_dict\n",
        "import contractions\n",
        "import unicodedata\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.externals import joblib\n",
        "import json\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "#stopwords.words('russian')\n",
        "import pymorphy2\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "from pyaspeller import YandexSpeller\n",
        "import nltk\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import pymorphy2\n",
        "from nltk.corpus import stopwords\n",
        "#from stop_words import get_stop_words\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import os \n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.externals import joblib\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFiKRSc0kY4g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install stop_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aGs-Nqigwsa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "from sklearn import datasets\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.externals import joblib\n",
        "import gensim\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC-nTSHr2yHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from stop_words import get_stop_words\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from stop_words import get_stop_words\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = list(get_stop_words('ru')) \n",
        "nltk_words = list(stopwords.words('russian')) \n",
        "stop_words.extend(nltk_words)\n",
        "from stop_words import safe_get_stop_words\n",
        "\n",
        "stop_word = safe_get_stop_words('unsupported language')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuZaa7GsXI4C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0a-yzvvXSir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('/content/drive/My Drive/lenta-ru-news.csv')\n",
        "#dataset = pd.read_csv('/content/drive/My Drive/lenta-ru-news.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiTXQyMogvan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=data[0:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbHAiO3yhO1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text=[w for w in df['text']]\n",
        "changes = {change['word']: change['s'][0] for change in speller.spell(text)}\n",
        "for word, suggestion in changes.items():\n",
        "    text = [ item.replace(word, suggestion) for item in text]\n",
        "df['text'] = pd.DataFrame(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIUvRgH3hOyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re \n",
        "r =  re.compile(\"[а-яА-Я]+\")\n",
        "def preprocessing(line):\n",
        "    line = line.lower()\n",
        "    line = re.sub(r\"[{}]\".format(string.punctuation), \" \", line)\n",
        "    return line\n",
        "def remove_num(text):\n",
        "    return(''.join(ch for ch in text if not ch.isdigit()))\n",
        "def remove_special_characters(text, remove_digits=False):\n",
        "    pattern = r'[^а-яА-Я+0-9\\s]' if not remove_digits else r'[^а-яА-Я+\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Saw6CVdshOvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.snowball import RussianStemmer\n",
        "df['text'] = df['text'].apply(remove_special_characters)\n",
        "#df['text']= df['text'].apply(lambda x: ''.join([stemmer.stem(word) for word in x]))\n",
        "#data['text'] = [w for w in filter(r.match, data['text'])]\n",
        "df['text']= df['text'].apply(preprocessing)\n",
        "#train = train.copy()\n",
        "df['text'] = df['text'].apply(lambda x: ' '.join([word for word in r.findall(x.lower())]))\n",
        "df['text'] =  df['text'].apply(lambda x:  ' '.join([contractions.fix(word) for word in x.split()]))\n",
        "df['text'] = df['text'].apply(remove_num)\n",
        "df['text'] =  df['text'].apply(lambda x: ''.join([word for word in x if word not in punctuation]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HQlZqT-kBpA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPcAQZLc83gV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['text'].str.len().hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9HekhEfpxHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['text_no_swords']=df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBAOjaBX9D9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['text_no_swords'].str.len().hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9XX8Bh94-RL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDl9FMvE-M8a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus=[]\n",
        "new= df['text_no_swords'].str.split()\n",
        "new=new.values.tolist()\n",
        "corpus=[word for i in new for word in i]\n",
        "counter=Counter(corpus)\n",
        "most=counter.most_common()\n",
        "x, y= [], []\n",
        "for word,count in most[:20]:\n",
        "    if (word not in stop_words):\n",
        "        x.append(word)\n",
        "        y.append(count)\n",
        "plt.figure(figsize=(20,10))\n",
        "sns.barplot(x=y,y=x,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxCt2V8_FHqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.groupby('topic').count()[['tags']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyijRmsq_YVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels=data['tags'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3lYxTiM_Jxg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(20,150))\n",
        "plt.title('Percentage of tags', fontsize=20)\n",
        "df.tags.value_counts().plot(kind='pie',\n",
        "                              wedgeprops=dict(width=.8), autopct='%1.0f%%', startangle= -20, \n",
        "                              textprops={'fontsize': 15})\n",
        "plt.legend(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bZ3Rz5CFbnV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns = list(data.groupby('topic').count()[['tags']].index)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "sns.set_context(\"notebook\", font_scale=1.5)\n",
        "sns.despine()\n",
        "\n",
        "ax = sns.barplot(x=columns, y='tags', data=data.groupby('topic').count()[['tags']], palette=(\"viridis\"))\n",
        "ax.set_title(label='topics', fontweight='bold', size=18, pad=20)\n",
        "plt.ylabel('Number of tags', fontsize=16)\n",
        "plt.xticks(rotation=90)\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwx6d4nLHLZk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nmf_x = data['topic'].value_counts()\n",
        "nmf_y = nmf_x.sort_index()\n",
        "plt.figure(figsize=(50,30))\n",
        "sns.barplot(nmf_x, nmf_y.index)\n",
        "plt.title(\"topic Distribution\", fontsize=50)\n",
        "plt.ylabel('Review Topics', fontsize=50)\n",
        "plt.yticks(fontsize=40)\n",
        "plt.xlabel('Frequency', fontsize=50)\n",
        "plt.xticks(fontsize=40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P11LstdvCw5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['length'] = df['text_no_swords'].apply(len)\n",
        "df['length'] = df['length'].replace(0,np.nan)\n",
        "df['length'].isnull().sum()\n",
        "df = df.dropna(subset=['length'])\n",
        "df['length'].plot(bins=50, kind='hist') \n",
        "plt.xlabel('words', fontsize=15)\n",
        "plt.ylabel('length of words in 1 text', fontsize=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3zK_t0v_JuY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.boxplot(x='length',data=df,palette='rainbow')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzXczCKQZM0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pymorphy2\n",
        "morph = pymorphy2.MorphAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzfFcDibfTkY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['token']=df['text_no_swords'].apply(word_tokenize)\n",
        "def normalForm(line):\n",
        "    return [morph.parse(word)[0].normal_form for word in line if len(word)>2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLIq57Svd-az",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['token'] = [normalForm(word) for word in df['token']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEwtVGHqESrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['token']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghveR8WV7w-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.util import ngrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dYbYOvdaSjJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_top_ngram(corpus):\n",
        "    vec = TfidfVectorizer(ngram_range=(2, 2)).fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) \n",
        "                  for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:50]\n",
        "top_n_bigrams=get_top_ngram(df['text_no_swords'])\n",
        "x,y=map(list,zip(*top_n_bigrams))\n",
        "plt.figure(figsize=(40,20))\n",
        "sns.barplot(x=y,y=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyFbjDguW85V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCNTlUoRUAG7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "detokenized_doc = []\n",
        "for i in range(len(df)):\n",
        "    t = ' '.join(df.token[i])\n",
        "    detokenized_doc.append(t)\n",
        "\n",
        "df['clean_doc'] = detokenized_doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsHXb_HlaSdZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer = TfidfVectorizer(max_features= 1000, max_df = 0.5,smooth_idf=True, ngram_range=(2, 2))\n",
        "\n",
        "X = vectorizer.fit_transform(df['clean_doc'])\n",
        "\n",
        "X.shape # check shape of the document-term matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRsfGx-wbdup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Vectorizer = TfidfVectorizer(ngram_range=(2,2))\n",
        "termFrequency = Vectorizer.fit_transform(df['clean_doc'])\n",
        "featureNames = Vectorizer.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY0p0-Zyohk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import NMF, LatentDirichletAllocation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d5DtBniqghZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display_topics(model, feature_names, no_top_words):\n",
        "    topic_dict = {}\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n",
        "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
        "        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n",
        "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
        "    return pd.DataFrame(topic_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCcSVV--vRrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn = TruncatedSVD(n_components=3).fit(termFrequency)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiUQloVgveKb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display_topics(trn, featureNames, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oCdxJI8yYQN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nmf = NMF(n_components=3, random_state=0, alpha=.1, l1_ratio=.5, init='nndsvd').fit(termFrequency)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3ogw8cwroX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display_topics(nmf, featureNames, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktxBwjrlbHIj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda = LatentDirichletAllocation(n_components=3, random_state=1,n_jobs=-1).fit(termFrequency)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wgyDhnNqi_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display_topics(lda, featureNames, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLhKv9hXrFCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Log likelihood: \", lda.score(termFrequency))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3PHr4iurE_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Perplexity: \", lda.perplexity(termFrequency))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3CRu-_erE87",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define Search Param\n",
        "search_params = {'n_components': [2, 3, 4, 5, 10, 15, 20, 25], 'learning_decay': [.5, .7, .9]}\n",
        "\n",
        "# Init the model\n",
        "lda = LatentDirichletAllocation()\n",
        "\n",
        "# Init Grid Search class\n",
        "model = GridSearchCV(lda, search_params)\n",
        "\n",
        "model.fit(termFrequency)\n",
        "best_lda_model = model.best_estimator_\n",
        "print(\"Best model's params: \", model.best_params_)\n",
        "print(\"Best log likelihood score: \", model.best_score_)\n",
        "print(\"Model perplexity: \", best_lda_model.perplexity(termFrequency))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6osRpJnpYru6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Best model's params:  {'learning_decay': 0.5, 'n_components': 2}\n",
        "Best log likelihood score:  -331679.02058652363\n",
        "Model perplexity:  1337.0782969491522"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}